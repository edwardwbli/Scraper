# -*- coding: utf-8 -*-
#Usage:
#scrapy crawl 51job -a qk='scrapy' -o item.csv
#qk for search keyword in 51Job
#
import scrapy
from scrapy.linkextractors import LinkExtractor
from scrapy_splash import SplashRequest
from jobsplash.items import Job51SearchItem


class Jobsearch(scrapy.Spider):
    name = "51job"
    allowed_domains = ["51job.com"]

    
    # http_user = 'splash-user'
    # http_pass = 'splash-password'
    #def start_request(self):
    #    for link in self.start_urls:
    #        yield SplashRequest(link,callback=sel.parse)
    def __init__(self,qk='python'):
        self.start_urls = ["http://search.51job.com/jobsearch/search_result.php?fromJs=1&jobarea=030200%2C00&funtype=0000&industrytype=00&keyword=" + qk]
    
    def parse(self, response):
        
        joblist =  response.xpath('//div[@class="dw_table"]/div[@class="el"]')
        for jobitem in joblist:
            #must create multiple items corresponding to every new scrapy request
            #if just create single item outside for loop
            #this item will overide until the end of the jobrequest list
            #yield to be a generator with same item reference,
            #
            item = Job51SearchItem()
            
            jobnames  = jobitem.xpath('p/span/a/@title').extract()
            companys  = jobitem.xpath('span[@class="t2"]/a/text()').extract()
            salarys   = jobitem.xpath('span[@class="t4"]/text()').extract()
            joblink   = jobitem.xpath('p/span/a/@href').extract()[0]
            item['jobname'] = jobnames
            item['company'] = companys
            item['salary'] = salarys
            jobrequest = scrapy.Request(joblink,callback=self.parse_job)
            jobrequest.meta['item'] = item
            yield jobrequest
        
        nextpage  = response.xpath('//div[@class="dw_page"]//li[@class="bk"]')[1]
        try:
            nextpagelink = nextpage.xpath('a/@href').extract()[0]
    
            yield  scrapy.Request(nextpagelink,callback=self.parse)
        except:
            print 'end of page'
        


    def parse_job(self, response):
   
        item = response.meta['item']
<<<<<<< Updated upstream
        content = [] 
=======
        content = []
>>>>>>> Stashed changes
        '''
        for text in response.xpath('//div[@class="bmsg job_msg inbox"]/text()').extract():
            content.append(text.strip())
        for text in response.xpath('//div[@class="bmsg job_msg inbox"]/p/text()').extract():
            content.append(text.strip())
        '''
        for text in response.xpath('//div[@class="bmsg job_msg inbox"]//text()').extract():
<<<<<<< Updated upstream
                        content.append(text.strip())   
=======
            content.append(text.strip())
>>>>>>> Stashed changes
        item['jobcont'] = ''.join(content).strip()
        return item
